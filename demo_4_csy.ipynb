{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aee0d91d",
   "metadata": {},
   "source": [
    "### Demo of how to use LLM API to do stuff\n",
    "\n",
    "`chat` is a function that takes in user prompt and returns LLM response.\n",
    "\n",
    "Before using it, you should set the *environment variables* `OPENAI_API_KEY` and `OPENAI_API_BASE`. Ask any LLM if you don't know how.\n",
    "\n",
    "**DO NOT** write out the key in any part of any file sent online. Use environment variable **only**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "993f7bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.shubiaobiao.cn/v1\n",
      "The current president of the United States is **Joe Biden**.\n"
     ]
    }
   ],
   "source": [
    "from api.api import chat\n",
    "import os\n",
    "\n",
    "print(os.environ[\"OPENAI_API_BASE\"])\n",
    "\n",
    "msg = chat(\"Who is the president of the US?\")\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf021692",
   "metadata": {},
   "source": [
    "Combining python string formatting and LLM api, we can easily automate things like:\n",
    "- summerizing text;\n",
    "- using LLM to do any particular type of judgement;\n",
    "- using LLM to give a grade in integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f1247fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hierarchical Reasoning Models (HRMs) achieve high performance on reasoning tasks but exhibit unexpected failure modes. Mechanistic analysis reveals that HRMs can fail on simple puzzles due to a violation of the fixed-point property, where the model does not maintain a correct solution once found. This instability is attributed to the one-step gradient training method, which prioritizes solving hard problems over stability on easier ones.\n",
      "\n",
      "Furthermore, HRMs display \"grokking\" dynamics in their reasoning steps, where the answer improves abruptly rather than incrementally. This suggests that the recursive process acts more like scaling \"guessing\" attempts than gradual refinement. The model can also get \"lost\" in its latent space, converging to incorrect \"spurious fixed points\" that act as misleading attractors.\n",
      "\n",
      "To address these issues and improve performance, the paper proposes scaling \"guess attempts\" through data augmentation (improving guess quality), input perturbation (increasing guess diversity via input transformations), and model bootstrapping (leveraging training randomness across checkpoints). Combining these strategies in \"Augmented HRM\" significantly boosts accuracy on Sudoku-Extreme from 54.5% to 96.9%, providing new insights into how reasoning models operate.\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/2601.10679/body.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "    prompt1 = f\"\"\"\n",
    "Summerize the content of the paper below into succint paragraphs. Give only your summerization, NO additional text.\n",
    "\n",
    "Paper content:\n",
    "{text}\n",
    "\"\"\"\n",
    "    summary = chat(prompt1)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f66eab0",
   "metadata": {},
   "source": [
    "Now we let an LLM evaluate the novelty of this paper.\n",
    "\n",
    "Use something like below as the prompt. Let the model wrap things like **scores** and **judgements** in certain brackets for easier extraction.\n",
    "\n",
    "In practice, it's better that we write more detailed prompts (like specifying what to consider when giving scores)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b113f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Novelty Evaluation\n",
      "\n",
      "**Key novel contributions:**\n",
      "- The mechanistic identification of the **fixed-point instability** in HRMs is a genuinely insightful finding — showing that models fail not because they can't find solutions but because they can't *maintain* them is a non-obvious and important observation.\n",
      "- The **reconceptualization of iterative reasoning as \"scaled guessing\"** rather than gradual refinement is a provocative and potentially paradigm-shifting insight into how these models actually operate. This challenges the intuitive narrative of \"thinking step by step.\"\n",
      "- The identification of **spurious fixed points** as misleading attractors in latent space provides useful theoretical grounding.\n",
      "\n",
      "**Limitations in novelty:**\n",
      "- The proposed remedies — data augmentation, input perturbation, and model ensembling/bootstrapping — are well-established techniques repackaged under the \"guess scaling\" framing. The engineering contribution is incremental.\n",
      "- The empirical scope is narrow (Sudoku), making it uncertain how well findings generalize to broader reasoning tasks (mathematical reasoning, code generation, etc.).\n",
      "- The grokking observation, while interesting in context, builds heavily on prior grokking literature without fundamentally extending it.\n",
      "- Analysis of iterative/recurrent reasoning models and their convergence properties has precedent in the looped transformer and deep equilibrium model literature.\n",
      "\n",
      "**Overall assessment:** The paper's primary novelty lies in its *diagnostic insights* rather than its *solutions*. The reframing of HRM reasoning as scaled guessing and the fixed-point instability analysis are genuinely novel conceptual contributions that could influence future model design. However, the narrow domain and conventional solutions temper the overall impact.\n",
      "\n",
      "<SCORE>5</SCORE>\n"
     ]
    }
   ],
   "source": [
    "prompt2 = f\"\"\"\n",
    "Below is a summerization of a paper in machine learning. Evaluate the novelty of this paper in integer scores from 1 to 10, the larger the better. Wrap the score in the pair <SCORE> and </SCORE>.\n",
    "\n",
    "Example: \n",
    "<SCORE>9<\\SCORE>\n",
    "\n",
    "Paper summary:\n",
    "{summary}\n",
    "\"\"\"\n",
    "ans = chat(prompt2, model=\"claude-opus-4-6-thinking\") # Stronger (but slower) model for important tasks.\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71155524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "from api.api import extract_score\n",
    "score = extract_score(ans)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
